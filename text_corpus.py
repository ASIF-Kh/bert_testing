text_corpus = """`DEVOPEDIA\n®\nfor developers. by developers.\n-\nText Corpus for NLP\n\n\n\n\n\n\nSummary\nDiscussion\nWhat are the traits of a good text corpus or wordlist?\nWhat are the different types of text corpora for NLP?\nWhat are the types of annotations that we can have on a text corpus?\nWhat are some NLP task-specific training corpora?\nCould you list some NLP text corpora by genre?\nWhat are some generic training corpora for NLP?\nDerived from text corpus, which datasets are useful for NLP tasks?\nWhich are some corpora for non-English languages?\nAre there curated lists of datasets for NLP work?\nWhere can I download text corpora for training NLP models?\nMilestones\nReferences\nFurther Reading\nArticle Stats\nCite As\nDatasets can help benchmark a model's performance. Source: Zhang and Wallace 2017, table 2. \n\nIn the domain of natural language processing (NLP), statistical NLP in particular, there's a need to train the model or algorithm with lots of data. For this purpose, researchers have assembled many text corpora. A common corpus is also useful for benchmarking models.\n\nTypically, each text corpus is a collection of text sources. There are dozens of such corpora for a variety of NLP tasks. This article ignores speech corpora and considers only those in text form.\n\nWhile English has many corpora, other natural languages too have their own corpora, though not as extensive as those for English. Using modern techniques, it's possible to apply NLP on low-resource languages, that is, languages with limited text corpora.\n\nDiscussion\nWhat are the traits of a good text corpus or wordlist?\n\nIt's said that a prototypical corpus must be machine-readable in Unicode. It must be a representative sample of the language in current use, balanced, and collected in natural settings.\n\nA good corpus or wordlist must have the following traits:\n\nDepth: A wordlist, for instance, should include the top 60K words and not just the top 3K words.\nRecent: Corpus based on outdated texts is not going to suit today's tasks.\nMetadata: Metadata should indicate the sources, assumptions, limitations and what's included in the corpus.\nGenre: Unless corpus has been collected for specific tasks, it should include different genres such as newspapers, magazines, blogs, academic journals, etc.\nSize: A corpus of half a million words or more ensures that low frequency words are also adequately represented.\nClean: A wordlist giving word forms of the same word can be messy to process. A better corpus would include only the lemma and part of speech.\nWhat are the different types of text corpora for NLP?\n\nA plain text corpus is suitable for unsupervised training. Machine learning models learn from the data in an unsupervised manner. However, a corpus that has the raw text plus annotations can be used for supervised training. It takes considerable effort to create an annotated corpus but it may produce better results.\n\nA corpus can be assembled from a variety of sources and genres. Such a corpus can be used for general NLP tasks. On the other hand, a corpus might be from a single source, domain or genre. Such a corpus can be used only for a specific purpose.\n\nWhat are the types of annotations that we can have on a text corpus?\nAmerican National Corpus Open annotated with POS, lemma and noun chunks, in XML and standalone form. Source: Gries and Berez 2017, fig. 6. \n\nPart-of-speech is one of the most common annotations because of its use in many downstream NLP tasks. Annotating with lemmas (base forms), syntactic parse trees (phrase-structure or dependency tree representations) and semantic information (word sense disambiguation) are also common. For discourse or text summarization tasks, annotations aid coreference resolutions. \n\nFor instance, British Component of the International Corpus of English (ICE-GB) of 1 million words is POS tagged and syntactically parsed. Another parsed corpus in Penn Treebank. While WordNet and FrameNet are not corpora, they contain useful semantic information.\n\nAudio/video recordings are transcribed and annotated as well. Annotations are phonetic (sounds), prosodic (variations), or interactional. Video transcripts may annotate for sign language and gesture.\n\nAnnotations could be inline/embedded with the text. When they appear on separate lines, it's called multi-tiered annotation. If they're in separate files, and linked to the text via hypertext, it's called standalone annotation.\n\nWhat are some NLP task-specific training corpora?\nExample questions and answers from SQuAD. Source: SQuAD 2019b. \n\nHere are some task-specific corpora:\n\nPOS Tagging: Penn Treebank's WSJ section is tagged with a 45-tag tagset. Use Ritter dataset for social media content.\nNamed Entity Recognition: CoNLL 2003 NER task is newswire content from Reuters RCV1 corpus. It considers four entity types. WNUT 2017 Emerging Entities task and OntoNotes 5.0 are other datasets.\nConstituency Parsing: Penn Treebank's WSJ section has dataset for this purpose.\nSemantic role labelling: OntoNotes v5.0 is useful due to syntactic and semantic annotations.\nSentiment Analysis: IMDb has released 50K movie reviews. Others are Amazon Customer Reviews of 130 million reviews, 6.7 million business reviews from Yelp, and Sentiment140 of 160K tweets.\nText Classification/Clustering: Reuters-21578 is a collection of news documents from 1987 indexed by categories. 20 Newsgroups is another dataset of about 20K documents from 20 newsgroups.\nQuestion Answering: Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset with 100K questions plus 50K unanswerable questions. Jeopardy dataset of about 200K Q&A is another example.\nCould you list some NLP text corpora by genre?\n\nFormal genre is typically from books and academic journals. Examples are Project Gutenberg EBooks, Google Books Ngrams, and arXiv Bulk Data Access. There are many text corpora from newswire. Examples are 20 Newsgroups and Reuters-21578.\n\nFor informal genre, we can include web data and emails. Corpora for these include Common Crawl, Blogger Corpus, Wikipedia Links Data, Enron Emails, and UCI's Spambase. Corpora derived from reviews include Yelp Reviews, Amazon Customer Reviews, and IMDb Movie Reviews. Even more informal are SMS and tweets, for which we have Sentiment140, Twitter US Airline Sentiment, and SMS Spam Collection.\n\nSpoken language is often different from written language. 2000 HUB5 English is a dataset that's a transcription of 40 telephone conversations. Signed language can also be annotated and transcribed to create a corpus.\n\nSince languages evolve, when analyzing old text, our models need to be trained likewise. Examples include DOE Corpus (600s-1150s), and COHA (1810s-2000s).\n\nAnother special case is of learners who are likely to express ideas differently. The Open Cambridge Learner Corpus contains 10K student responses of 2.9 million words.\n\nIt's also common to have domain-specific corpora. For example, BioCreative and GENIA are for biology.\n\nWhat are some generic training corpora for NLP?\n\nSome of the well-known corpora are Brown Corpus, British National Corpus (BNC), Lancaster-Oslo/Beren Corpus (LOB), International Corpus of English (ICE), Corpus of Contemporary American English (COCA), Google Books Ngram Corpus, Penn Treebank-3, English Gigaword Fifth Edition, and OntoNotes Release 5.0.\n\nWikipedia was not made for training NLP models but it can be used. We would need to strip markup. Gensim Python package has gensim.corpora.wikicorpus.WikiCorpus class to process Wikipedia data.\n\nGeneric corpora are usually suited for language modelling, which is useful for other downstream tasks such as machine translation and speech recognition. Researchers have suggested using Project Gutenberg EBooks; Penn Treebank of about a million words pre-processed by Mikolov et al. in 2011; WikiText-2 of more than 2 million words; and WikiText-103. Google's one-billion word corpus provides a useful benchmark.\n\nDerived from text corpus, which datasets are useful for NLP tasks?\n\nWordlists such as list of names or stopwords are useful for NLP work. Phrases in English (PIE) is another resource to explore distribution of words and phrases. It's based on the BNC corpus.\n\nTagsets are essential for POS tagging, chunking, dependency parsing or constituency parsing. DKPro Core Tagset Reference is an excellent resource. University of Lancaster maintains a multilingual semantic tagset.\n\nTreebanks go beyond just POS-tagging a corpus. A treebank is an annotated corpus in which grammatical structure is typically represented as a tree structure. Examples are Penn Treebank and CHRISTINE Corpus. Treebanks are useful for evaluating syntactic parsers or as resources for ML models to optimize linguistic analyzers.\n\nWord embeddings are real-valued vectors representations of words. These have improved many NLP task including language modelling and semantic analysis. While it's possible to learn embeddings from a large corpus, it's easier to start with downloadable embeddings. Two sources for downloads are Polyglot and Nordic Language Processing Laboratory (NLPL).\n\nPerhaps by 2020, we'll be able to download pretrained language models and apply it to a variety of NLP tasks.\n\nWhich are some corpora for non-English languages?\n\nFor machine translation, it's common to have parallel corpus, that is, aligned text in multiple languages. We mention a few examples:\n\nAligned Hansards of the 36th Parliament of Canada containing 1.3 million pairs of aligned text segments in English and French\nEuroparl parallel corpus from 1996-2011 of 21 European languages from parliament proceedings\nWMT 2014 EN-DE and WMT 2014 EN-FR\nA corpus using Wikipedia across 20 languages, 36 bitexts, about 610 million tokens and 26 million sentence fragments\n\nAn excellent source is OPUS, the open parallel corpus. Lionbridge published a list of parallel corpora in 2019. Martin Weisser maintains a list that links to many non-English corpora.\n\nAre there curated lists of datasets for NLP work?\n\nA simple web search will yield plenty of relevant results. Some include download links to the sources. We mention a few that stand out:  \n\nLinguistic Data Consortium has a list of corpora grouped by project\nEnglish Corpora hosts nine large corpora\nThe Stanford NLP Group has shared a list of corpora and treebanks\nRegistry of Open Data on AWS stores some NLP-specific datasets\nNLP datasets at fast.ai is actually stored on Amazon S3\nShared by users, data.world lists 30+ NLP datasets\nShared by users, Kaggle list wordlists, embeddings and text corpora\nNicolas Iderhoff's list of NLP datasets includes collection dates and dataset sizes\nSebastian Ruder tracks NLP progress, organized by tasks, with links to external datasets\nMartin Weisser maintains a list of historical and diachronic corpora\nIn NLTK Python code, call nltk.download() but we can download them separately as well\nFrom blogs, three separate lists are from Cambridge Spark, Lionbridge and Open Data Science\nWhere can I download text corpora for training NLP models?\n\nThese are the download links for some notable text corpora:  \n\nBrown Corpus\nCorpus of Contemporary American English (COCA)\nPenn Treebank-3 (paid)\nData dumps of English Wikipedia\nWikipedia Links Data\nProject Gutenberg EBooks\nGoogle Books Ngrams via Google or via Amazon S3 bucket\narXiv Bulk Data Access\nCommon Crawl\nDBpedia 3.5.1 Knowledge Base\nAmazon Customer Reviews\nIMDb Reviews\nGoogle Blogger Corpus\nJeopardy Question-Answer Dataset\nYelp Open Dataset\nEnron Email Dataset\n20 Newsgroups\nSentiment140\nSMS Spam Collection\nWordNet\nMilestones\n1964\n\nW. Nelson Francis and Henry Kučera at the Department of Linguistics, Brown University, publish a computer-readable general corpus to aid linguistic research on modern English. The corpus has 1 million words (500 samples of about 2000 words each). Revised editions appear later in 1971 and 1979. Called Brown Corpus, it inspires many other text corpora. The corpus with annotations is included in Treebank-3 (1999).\n\n1992\n\nLinguistic Data Consortium (LDC) is formed to serve as a repository for NLP resources, including corpora. It's hosted at the University of Pennsylvania.\n\n1994\nBNC is a balanced corpus. Source: Wikipedia 2019b. \n\nA 100-million corpus of British English called BNC (British National Corpus) is assembled between 1991 and 1994. It's balanced across genres. A follow-up task called BNC2014 is started in 2014, which can help in understanding how language evolves. Spoken BNC2014 is released in September 2017. Written BNC2014 is expected to come out in 2019.\n\n1999\n\nPenn Treebank-3 is released. It's based upon the original Treebank (1992) and its revised Treebank II (1995). This work started in 1989 at the University of Pennsylvania. Treebank-3 includes tagged/parsed Brown Corpus, 1 million words of 1989 WSJ material annotated in Treebank II style, tagged sample of ATIS-3, and tagged/parsed Switchboard Corpus. Apart from POS tags, the corpus includes chunk tags, relation tags and anchor tags. The BLLIP 1987-89 WSJ Corpus Release 1 has 30 million words and supplements the WSJ section of Treebank-3.\n\n2008\n\nCollected for the years 1990-2007, the Corpus of Contemporary American English (COCA) is released with 365 million words. By December 2017, it has 560 million words, adding 20 million each year. There's good balance of spoken, fiction, popular magazines, newspapers, and academic texts. It's been noted that COCA contains many common words that are missing in the American National Corpus (ANC), a corpus of 22 million words. \n\nJun\n2011\n\nEnglish Gigaword Fifth Edition is released by LDC. It comes from seven English newswire services. It has 4 billion words and takes up 26 gigabytes uncompressed. The first edition appeared in 2003. In November 2012, researchers at the John Hopkins University add syntactic and discourse structure annotations to this corpus after parsing more than 183 million sentences.\n\nJul\n2012\n\nFrom digitized books, Google releases version 2 of Google Books Ngrams. Version 1 came out in July 2009. Only n-grams that appear more than 40 times are included. The corpus includes 1-gram to 5-grams. It includes many non-English languages as well. To experiment on small sets of phrases, researchers can try out the online Google Books Ngram Viewer.\n\nAug\n2012\n\nAs a corpus for informal genre, English Web Treebank (EWT) is released by LDC. This includes content from weblogs, reviews, question-answers, newsgroups, and email. It has about 250K word-level tokens and 16K sentence-level tokens. It's annotated for POS and syntactic structure. This includes Enron Corporation emails from 1999-2002. In 2014, Silveira et al. provide annotation of syntactic dependencies for this corpus that can be used to train dependency parsers.\n\nSep\n2019\n\nCommon Crawl publishes 240 TiB of uncompressed data from 2.55 billion web pages. Of these, 1 billion URLs were not present in previous crawls. Common Crawl started in 2008. In 2013, they moved from ARC to Web ARChive (WARC) file format. WAT files contain the metadata. WET file contain plaintext of the WARC files.\n\nReferences\nAl-Rfou, Rami. 2019. "Polyglot." Via Google Sites. Accessed 2019-10-28. \nAli, Meiryum. 2019. "The Best 25 Datasets for Natural Language Processing." Lionbridge, July 09. Accessed 2019-10-23. \nAli, Meiryum. 2019b. "25 Best Parallel Text Datasets for Machine Translation Training." Lionbridge, June 07. Accessed 2019-10-25. \nBYU. 2019. "The Corpus of Contemporary American English (COCA) and the American National Corpus (ANC)." Brigham Young University. Accessed 2019-10-25. \nBarba, Paul. 2019. "Machine Learning for Natural Language Processing." Blog, Lexalytics, March 25. Accessed 2019-10-28.\nBies, Ann, Justin Mott, Colin Warner, and Seth Kulick. 2012. "English Web Treebank." LDC2000T43, Philadelphia: Linguistic Data Consortium August 16. Accessed 2019-10-24. \nBioCreative. 2019. "Biology corpora." Accessed 2019-10-25. \nBoukkouri, Hicham El. 2018. "Text Classification: The First Step Toward NLP Mastery." Data From The Trenches, on Medium, June 18. Accessed 2019-10-23. \nBrownlee, Jason. 2017. "Datasets for Natural Language Processing." Machine Learning Mastery, September 27. Updated 2019-08-07. Accessed 2019-10-23. \nCASS. 2019. "BNC2014." ESRC Centre for Corpus Approaches to Social Science (CASS). Accessed 2019-10-25. \nCLiPS. 2018. "Penn Treebank II tag set." CLiPS Research Center, June 22. Accessed 2019-09-07.\nCambridge Spark. 2018. "50 free Machine Learning datasets: Sentiment Analysis." Cambridge Spark, on Medium, August 29. Accessed 2019-10-23. \nCharniak, Eugene, Don Blaheta, Niyu Ge, Keith Hall, John Hale, and Mark Johnson. 2000. "BLLIP 1987-89 WSJ Corpus Release 1." LDC2000T43, Philadelphia: Linguistic Data Consortium. Accessed 2019-10-24. \nCommon Crawl. 2019. "So you’re ready to get started." Common Crawl. Accessed 2019-10-25. \nDKPro Core. 2017. "DKPro Core™ Tagset Reference." Version 1.9.0, December 23. Accessed 2019-10-28. \nEnglish Corpora. 2019. "Corpus of Contemporary American English." Accessed 2019-10-25. \nEnglish Corpora. 2019b. "Corpus of Historical American English." Accessed 2019-10-25. \nEvans, David. 2019. "Corpus building and investigation for the Humanities." University of Birmingham. Accessed 2019-10-28. \nFletcher, William H. 2011. "Phrases in English Home." June 28. Accessed 2019-10-28. \nGoogle Books. 2012. "Ngram Viewer." Google Books. Accessed 2019-10-25. \nGraff, David, and Christopher Cieri. 2003. "English Gigaword." LDC2003T05, Philadelphia: Linguistic Data Consortium. Accessed 2019-10-24. \nGries, Stefan Th. and Andrea L. Berez. 2017. "Linguistic Annotation in/for Corpus Linguistics." Chapter in: N. Ide and J. Pustejovsky (eds.), Handbook of Linguistic Annotation, Springer Science+Business Media Dordrecht, pp. 379-409. Accessed 2019-10-23. \nGrigaliūnienė, Jonė. 2015. "Corpora in the Classroom." Vilniaus Universitetas. Accessed 2019-10-24. \nGurulingappa, Harsha, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo. 2012. "Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports." Journal of Biomedical Informatics, Elsevier, vol. 45, no. 5, pp. 885-892, October. Accessed 2019-10-28. \nHealey, Antonette diPaolo. 2009. "Dictionary of Old English Corpus (DOE Corpus)." CoRD, Varieng, November 23. Updated 2011-03-17. Accessed 2019-10-25. \nIderhoff, Nicolas. 2020. "nlp-datasets." niderhoff/nlp-datasets, June 29. Accessed 2020-07-24. \nJohnston, Trevor. 2008. "From archive to corpus: transcription and annotation in the creation of signed language corpora." Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation, pp. 16-29, November. Accessed 2019-10-25. \nKauhanen, Henri. 2011. "The Standard Corpus of Present-Day Edited American English (the Brown Corpus)." VARI­ENG, University of Helsinki, March 20. Accessed 2019-09-06. \nLDC. 2019. "About LDC." Linguistic Data Consortium. Accessed 2019-10-24. \nMarcus, Mitch. 2011. "A Brief History of the Penn Treebank." Center for Language and Speech Processing, Johns Hopkins University, February 15. Accessed 2019-09-06. \nMarcus, Mitchell P., Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. "Treebank-3." LDC99T42, Philadelphia: Linguistic Data Consortium. Accessed 2019-10-24. \nMayo, Matthew. 2017. "Building a Wikipedia Text Corpus for Natural Language Processing." KDnuggets, November. Accessed 2019-10-23. \nNLTK. 2019. "Corpus Readers." NLTK. Accessed 2019-10-23. \nNagel, Sebastian. 2019. "September 2019 crawl archive now available." Common Crawl, September 28. Accessed 2019-10-25. \nNapoles, Courtney, Matthew Gormley, and Benjamin Van Durme. 2012. "Annotated English Gigaword." LDC2012T21, Philadelphia: Linguistic Data Consortium, November 15. Accessed 2019-10-24. \nNarasimhan, Karthik. 2017. "Datasets for Natural Language Processing." karthikncode/nlp-datasets, August 7. Accessed 2020-07-24. \nNivre, Joakim. 2007. "Treebanks." Chapter 11 in: Corpus Linguistics, Handbooks of Linguistics and Communication Science (HSK), De Gruyter. Accessed 2019-10-28.\nO'Donnell, Matthew Brook. 2008. "Corpus Mark-up." UoL Summer Institute in Corpus Linguistics, July 01. Accessed 2019-10-23.\nODSC. 2019. "20 Open Datasets for Natural Language Processing." Medium, July 31. Accessed 2019-10-25. \nOPUS. 2019. "Wikipedia." The Open Parallel Corpus. Accessed 2019-10-25. \nParker, Robert, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. "English Gigaword Fifth Edition." LDC2011T07, Philadelphia: Linguistic Data Consortium, June 17. Accessed 2019-10-24. \nQuantum Stat. 2020. "The Big Bad NLP Database." Quantum Stat, June 25. Accessed 2020-07-24. \nRegistry of Open Data. 2019. "Amazon Customer Reviews Dataset." Registry of Open Data on AWS. Accessed 2019-10-24.\nRennie, Jason. 2008. "20 Newsgroup." January 14. Accessed 2019-10-24. \nRuder, Sebastian. 2018a. "NLP's ImageNet moment has arrived." July 12. Accessed 2019-10-23. \nRuder, Sebastian. 2018b. "Semantic role labeling." NLP-progress, on GitHub, October 25. Accessed 2019-10-24. \nRuder, Sebastian. 2019a. "Constituency parsing." NLP-progress, on GitHub, September 14. Accessed 2019-10-24. \nRuder, Sebastian. 2019b. "Machine translation." NLP-progress, on GitHub, February 25. Accessed 2019-10-24. \nRuder, Sebastian. 2019c. "Part-of-speech tagging." NLP-progress, on GitHub, September 24. Accessed 2019-10-24. \nRuder, Sebastian. 2019d. "Named entity recognition." NLP-progress, on GitHub, April 30. Accessed 2019-10-24. \nRuder, Sebastian. 2019e. "Language modeling." NLP-progress, on GitHub, October 06. Accessed 2019-10-24. \nSQuAD. 2019. "SQuAD 2.0: The Stanford Question Answering Dataset." Stanford NLP Group. Accessed 2019-10-23. \nSQuAD. 2019b. "Black_Death." SQuAD 2.0, Stanford NLP Group. Accessed 2019-10-23. \nSilveira, Natalia, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning. 2014. "A Gold Standard Dependency Corpus for English." Proceedings of the Ninth International Conference on Language Resources and Evaluation, European Language Resources Association (ELRA), pp. 2897-2904, May. Accessed 2019-10-25. \nSketch Engine. 2017. "Open Cambridge Learner Corpus (Uncoded)." Sketch Engine, March 15. Updated 2019-06-18. Accessed 2019-10-25. \nStanford NLP. 2019a. "Coreference Resolution." The Stanford Natural Language Processing Group. Accessed 2019-10-23. \nUCREL. 2019. "UCREL Semantic Analysis System (USAS)." UCREL, University of Lancaster. Accessed 2019-10-28. \nWeischedel, Ralph, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert Belvin, and Ann Houston. 2013. "OntoNotes Release 5.0." LDC2013T19, Philadelphia: Linguistic Data Consortium, October 13. Accessed 2019-10-24. \nWikipedia. 2019. "List of text corpora." Wikipedia, July 26. Accessed 2019-10-24. \nWikipedia. 2019b. "British National Corpus." Wikipedia, October 22. Accessed 2019-10-24. \nWord Frequency. 2019. "Word frequency data." Accessed 2019-10-23. \nYelp. 2019. "Yelp Open Dataset." Accessed 2019-10-24. \nZhang, Ye and Byron C. Wallace. 2017. "A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification." Proceedings of the 8th International Joint Conference on Natural Language Processing, pp. 253–263, Taipei, Taiwan, November 27 – December 1. Accessed 2019-10-23. \nFurther Reading\nIderhoff, Nicolas. 2020. "nlp-datasets." niderhoff/nlp-datasets, June 29. Accessed 2020-07-24.\nAli, Meiryum. 2019. "The Best 25 Datasets for Natural Language Processing." Lionbridge, July 09. Accessed 2019-10-23.\nBrownlee, Jason. 2017. "Datasets for Natural Language Processing." Machine Learning Mastery, September 27. Updated 2019-08-07. Accessed 2019-10-23.\nGries, Stefan Th. and Andrea L. Berez. 2017. "Linguistic Annotation in/for Corpus Linguistics." Chapter in: N. Ide and J. Pustejovsky (eds.), Handbook of Linguistic Annotation, Springer Science+Business Media Dordrecht, pp. 379-409. Accessed 2019-10-23.\nArticle Stats\n2201\nWords\n\n1\nAuthors\n\n5\nEdits\n\n2\nChats\n\n4\nLikes\n\n43K\nHits\n\nCite As\nDevopedia. 2020. "Text Corpus for NLP." Version 5, December 20. Accessed 2023-05-02. https://devopedia.org/text-corpus-for-nlp\nCOPY CITATION\nContributed by\n1 author\n\nLast updated on\n20 Dec 2020\ndata natural language processing modelling dataset\nSee Also \nSpeech Corpus for NLP \nNeural Networks for NLP\nWord Embedding\nOpen Data\nStructured vs Unstructured Data\nData Mining \nABOUT\nTERMS OF USE\nPRIVACY POLICY\nFOUNDATION\nTRUSTEES\nDONATIONS\nMISSION\nVALUES\nLICENSING\nEVENTS\nREPORT ISSUES\nOPEN SOURCE`"""


import re

def preprocess_text(text):
    # Remove special characters, numbers, and punctuation
    text = re.sub(r'[^A-Za-z\s]', '', text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove extra whitespaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# Example usage:
# raw_text = "Hello, this is an example sentence with 123 numbers!!!"
raw_text = text_corpus
preprocessed_text = preprocess_text(raw_text)

with open("refined_text_corpus.txt", "w") as refined_text_corpus:
    refined_text_corpus.write(preprocessed_text)




